ğŸ“‰ Gradient Descent Visualization for Linear Regression

ğŸ“Œ Project Overview

This project demonstrates how Gradient Descent updates the intercept (b) in Simple Linear Regression.

The objective is to understand how the cost function derivative adjusts the regression line step-by-step until it approaches the Optimal Least Squares (OLS) solution.

Instead of directly using the final model, this project visually shows how the regression line moves during each iteration.

ğŸ“ Dataset

The dataset is generated using:

from sklearn.datasets import make_regression

Parameters used:

n_samples = 4

n_features = 1

noise = 80

random_state = 13

This creates a simple synthetic regression dataset with noise.

ğŸ§  Algorithm Concept
ğŸ”¹ Model Equation

y = mx + b

Where:

m = slope

b = intercept

ğŸ”¹ Gradient Descent Update Rule

To update intercept (b), we compute the derivative of the cost function:

âˆ‚J/âˆ‚b = -2 Î£ (y - mx - b)

Update rule:

b = b - Î± * âˆ‚J/âˆ‚b

Where:

Î± = learning rate

âˆ‚J/âˆ‚b = gradient

b = updated intercept

âš™ï¸ Implementation Details

Slope (m) is initialized from OLS result (~78.35)

Intercept (b) starts from 0

Learning rate (lr) = 0.1

Multiple iterations are performed

Regression line is plotted after each update

The model gradually moves toward the optimal OLS solution.

ğŸ“Š Visualization

The graph contains:

ğŸ”´ Red Line â†’ OLS solution (from sklearn)

ğŸ”µ Blue Line â†’ Initial line (b = 0)

ğŸŸ¢ Green/Black/Yellow Lines â†’ Updated lines after each iteration

This clearly shows how Gradient Descent shifts the line upward until it matches OLS.

ğŸ“ˆ Results

From sklearn OLS:

Slope (m):
78.3506

Intercept (b):
26.1596

After multiple Gradient Descent updates, the intercept converges close to:

b â‰ˆ 26.11

This confirms that Gradient Descent successfully approaches the optimal solution.
